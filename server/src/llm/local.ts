// Local LLM integration (Ollama, Llama.cpp, etc.)
// Loads endpoint from process.env.LOCAL_LLM_URL

export async function callLocalLLM(model: string, messages: any[], options: any = {}) {
  // TODO: Implement local LLM call
  // Return response in { role, content } format
} 