# Render Blueprint for AI Chatbot Backend
# Frontend is deployed separately on Vercel

services:
  # --- Backend API Service ---
  - type: web
    name: ai-chatbot-backend
    env: docker
    autoDeploy: true
    dockerfilePath: ./server/Dockerfile
    buildCommand: npm ci --only=production
    startCommand: npm start
    buildFilter:
      paths:
        - server/**
    envVars:
      - key: NODE_ENV
        value: production
      - key: PORT
        value: 3001
      - key: OPENAI_API_KEY
        sync: false # Marked as secret, set in Render dashboard
      - key: OPENAI_MODEL
        value: gpt-3.5-turbo
      - key: OPENAI_EMBEDDING_MODEL
        value: text-embedding-ada-002
      - key: LOG_LEVEL
        value: info
      - key: CORS_ORIGIN
        value: https://your-frontend-name.vercel.app
      - key: SUPABASE_URL
        value: '' # Set in Render dashboard
      - key: SUPABASE_SERVICE_ROLE_KEY
        sync: false # Marked as secret, set in Render dashboard
      # Optional/secure backend env vars
      - key: REDIS_URL
        value: '' # Set in Render dashboard if using Redis
      - key: CACHE_TTL_SECONDS
        value: '3600'
      - key: MAX_CACHE_SIZE
        value: '1000'
      - key: ENABLE_MEMORY_CACHE
        value: 'true'
      - key: JWT_SECRET
        sync: false # Marked as secret, set in Render dashboard
      - key: ENABLE_HELMET
        value: 'true'
      - key: ENABLE_COMPRESSION
        value: 'true'
      - key: ENABLE_REQUEST_LOGGING
        value: 'true'
      - key: RATE_LIMIT_WINDOW_MS
        value: '900000'
      - key: RATE_LIMIT_MAX_REQUESTS
        value: '100'
      # Add other backend env vars as needed

  # --- Redis Service (Optional, for caching) ---
  - type: redis
    name: ai-chatbot-redis
    plan: free
    maxmemoryPolicy: allkeys-lru
    ipAllowList: [] # Allow all services in this Render blueprint to connect