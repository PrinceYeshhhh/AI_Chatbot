version: '3.8'

services:
  # Frontend (React + Vite)
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
      target: runner
    ports:
      - "5173:80"
    environment:
      - NODE_ENV=production
      - VITE_API_URL=http://localhost:3001
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - ai-chatbot-network
    depends_on:
      - backend

  # Backend (Node.js + Express + AI/ML)
  backend:
    build:
      context: ./server
      dockerfile: Dockerfile
      target: production
    ports:
      - "3001:3001"
    environment:
      - NODE_ENV=production
      - PORT=3001
      - GROQ_API_KEY=${GROQ_API_KEY}
      - TOGETHER_API_KEY=${TOGETHER_API_KEY}
      - GROQ_MODEL=${GROQ_MODEL:-llama3-70b-8192}
      - TOGETHER_EMBEDDING_MODEL=${TOGETHER_EMBEDDING_MODEL:-togethercomputer/m2-bert-80M-8k-base}
      - CHROMA_DB_PATH=/app/vector_store
      - UPLOAD_DIR=/app/uploads
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - CORS_ORIGIN=http://localhost:5173
      - ENABLE_MEMORY_CACHE=true
      - CACHE_TTL_SECONDS=3600
      - REDIS_URL=${REDIS_URL:-redis://redis:6379}
    volumes:
      - backend_uploads:/app/uploads
      - backend_vector_store:/app/vector_store
      - backend_logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - ai-chatbot-network
    depends_on:
      redis:
        condition: service_healthy

  # Redis for caching (recommended for production)
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - ai-chatbot-network
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru

  # Nginx reverse proxy for production
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx-full.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
    depends_on:
      - frontend
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai-chatbot-network

  # Monitoring with Prometheus (optional)
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - ai-chatbot-network

volumes:
  backend_uploads:
    driver: local
  backend_vector_store:
    driver: local
  backend_logs:
    driver: local
  redis_data:
    driver: local
  nginx_logs:
    driver: local
  prometheus_data:
    driver: local

networks:
  ai-chatbot-network:
    driver: bridge